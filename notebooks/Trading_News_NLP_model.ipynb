{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing for Signal Generation on News Data\n",
    "\n",
    "In the following code,  we will utilize pretrained embeddings from both GloVe and FastText Skipgram models to preprocess text datasets for LSTM network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages and Initialize the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import datetime\n",
    "import pydot, graphviz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import date\n",
    "from numpy.random import seed\n",
    "from IPython.display import Image\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import set_random_seed\n",
    "from tensorflow.python import pywrap_tensorflow\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM, concatenate, Bidirectional\n",
    "from keras.layers import PReLU, ELU, LeakyReLU, GRU, SimpleRNN\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Embedding, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/marketlab/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tradingcore.utils import embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting random state to eliminate randomness. Assigning constant variables for later uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.2.4\n",
      "Tensorflow version: 1.13.1\n",
      "Sklearn version: 0.20.1\n"
     ]
    }
   ],
   "source": [
    "seed(42)\n",
    "set_random_seed(42)\n",
    "MAX_SEQUENCE_LENGTH = 32\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "print(\"Keras version:\",keras.__version__)\n",
    "print(\"Tensorflow version:\",tf.__version__)\n",
    "print(\"Sklearn version:\",sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data - Financial News Dataset \n",
    "\n",
    "We will be utilizing open source news data from Bloomberg and Reuters between 2006 and 2012.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "      <th>tldr</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exxon Mobil offers plan to end Alaska dispute</td>\n",
       "      <td>2006-10-20 06:15:00</td>\n",
       "      <td>http://www.reuters.com/article/2006/10/20/busi...</td>\n",
       "      <td>In a proposal sent earlier this week to the Al...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey buddy, can you spare $600 for a Google share?</td>\n",
       "      <td>2006-10-20 04:25:00</td>\n",
       "      <td>http://www.reuters.com/article/2006/10/20/busi...</td>\n",
       "      <td>SAN FRANCISCO/NEW YORK  (Reuters) - Wall Stree...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ford posts biggest loss in 14 years</td>\n",
       "      <td>2006-10-23 06:42:00</td>\n",
       "      <td>http://www.reuters.com/article/2006/10/23/us-a...</td>\n",
       "      <td>Ford also said it was considering raising new ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shell looks to buy out Canada unit for C$7.7 b...</td>\n",
       "      <td>2006-10-23 04:34:00</td>\n",
       "      <td>http://www.reuters.com/article/2006/10/23/us-e...</td>\n",
       "      <td>In July, Shell Canada rattled the industry and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. venture investors betting on energy, Web 2.0</td>\n",
       "      <td>2006-10-23 08:36:00</td>\n",
       "      <td>http://www.reuters.com/article/2006/10/23/us-f...</td>\n",
       "      <td>SAN FRANCISCO  (Reuters) - U.S. venture capita...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline            timestamp  \\\n",
       "0      Exxon Mobil offers plan to end Alaska dispute  2006-10-20 06:15:00   \n",
       "1  Hey buddy, can you spare $600 for a Google share?  2006-10-20 04:25:00   \n",
       "2                Ford posts biggest loss in 14 years  2006-10-23 06:42:00   \n",
       "3  Shell looks to buy out Canada unit for C$7.7 b...  2006-10-23 04:34:00   \n",
       "4  U.S. venture investors betting on energy, Web 2.0  2006-10-23 08:36:00   \n",
       "\n",
       "                                                 url  \\\n",
       "0  http://www.reuters.com/article/2006/10/20/busi...   \n",
       "1  http://www.reuters.com/article/2006/10/20/busi...   \n",
       "2  http://www.reuters.com/article/2006/10/23/us-a...   \n",
       "3  http://www.reuters.com/article/2006/10/23/us-e...   \n",
       "4  http://www.reuters.com/article/2006/10/23/us-f...   \n",
       "\n",
       "                                                tldr  Class  \n",
       "0  In a proposal sent earlier this week to the Al...      2  \n",
       "1  SAN FRANCISCO/NEW YORK  (Reuters) - Wall Stree...      1  \n",
       "2  Ford also said it was considering raising new ...      1  \n",
       "3  In July, Shell Canada rattled the industry and...      1  \n",
       "4  SAN FRANCISCO  (Reuters) - U.S. venture capita...      1  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/news_data/news_data_labelled.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting timestamp: 2006-10-20 04:25:00\n",
      "Ending timestamp: 2012-12-31 23:06:28\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting timestamp: {}\".format(df.timestamp.min()))\n",
    "print(\"Ending timestamp: {}\".format(df.timestamp.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the dataset into testing and training datasets for machine learning**\n",
    "* X and y respectively correspond to data features (i.e. input) and data labels (i.e. output)\n",
    "  * Training set the data used to \"learn\" the parameters in our model with a supervised learning method. This usually uses the majority of the original dataset to achieve best effect.\n",
    "  * Testing set is the data used to evaluate the effectiveness of our model, often used to produce numerical metrics (e.g. accuracy rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.tldr\n",
    "y = df.Class\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    stratify = y,\n",
    "                                                    test_size=0.10,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "<img src=\"../imgs/preprocess_data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenize training set**\n",
    "\n",
    "* Tokenizer from Keras creates a vocabulary index from the training set based on word frequency.\n",
    "  * Tokenize here also did special character removal for us\n",
    "  * we can also perform stop word removal here\n",
    "* Every unique word is assigned a unique integer value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_filter = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "\n",
    "tokenizer = Tokenizer(num_words = None,\n",
    "                      filters = word_filter,\n",
    "                      lower = True,\n",
    "                      split = \" \",\n",
    "                      char_level = False)\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert all datasets to numerical values**\n",
    "* Apply the vocabulary index to X_train and X_test\n",
    "  * The datasets are converted from texts to sequences of integers based on previously created vocabulary index\n",
    "  * The sequences are padded with zeros and are limited with MAX_SEQUENCE_LENGTH to have a fixed length\n",
    "* Convert y_train and y_test to one-hot encoded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(tokenizer.texts_to_sequences(X_train),\n",
    "                        maxlen = MAX_SEQUENCE_LENGTH,\n",
    "                        value = 0.0)\n",
    "\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(X_test),\n",
    "                       maxlen = MAX_SEQUENCE_LENGTH,\n",
    "                       value = 0.0)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Embeding Matrix\n",
    "\n",
    "In this project, we will use pretrained word embeding which are stored in files. We have to build the matrices from these files before we use them. Vocabulary index created earlier with the tokenizer is used to create the embedding matrices **embedding_matrix** is a function defined in utils.py which helps us build word embeding from files. It has the following parameters:\n",
    "* path_to_embeddingï¼š path to text file of word embeddings\n",
    "* embedding_dim: dimension of word embeddings\n",
    "* word_index: dictionary mapping words to indices\n",
    "\n",
    "The output is a numpy matrix containing the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding_matrix = embedding_matrix(\"../data/news_data/glove/glove.840B.300d.txt\",\n",
    "                                          EMBEDDING_DIM,\n",
    "                                          tokenizer.word_index)\n",
    "\n",
    "fasttext_embedding_matrix = embedding_matrix(\"../data/news_data/fasttext/wiki-news-300d-1M.vec\",\n",
    "                                             EMBEDDING_DIM,\n",
    "                                             tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks Model\n",
    "Recall from the first notebook that Nerual Network can be treated as a Composite function and each layer is one of these ingredient functions. When we write code to build our own Nerual Network using Keras, what we have to do is just find the layer we want to use and compose (or say concatenate) them. For example to represent the following network:\n",
    "<img src=\"../imgs/layer_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Network for processing sequential information**\n",
    "This function build the subnet for processing sequential information. In this project, the input of this subnet is our embeded word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subchannel network for encoding sequential information\n",
    "def subnetwork_channel(input_layer : layers, RNN_architecture : str, units : int, dropout_rate : float) -> layers:\n",
    "    \"\"\"\n",
    "    This function creates a sub network for encoding sequences.\n",
    "    \n",
    "    Inputs:\n",
    "    input_layer - The input keras layer into the subnetwork\n",
    "    RNN_architecture - Name of the RNN type to use\n",
    "    units - Number of units in the RNN\n",
    "    dropout_rate - dropout rate\n",
    "    \n",
    "    Outputs:\n",
    "    batch - Batch Normalized output layer\n",
    "    \n",
    "    \"\"\"\n",
    "    assert RNN_architecture in [\"LSTM\", \"GRU\", \"RNN\"]\n",
    "    \n",
    "    dropout1 = Dropout(rate = dropout_rate)(input_layer)\n",
    "    \n",
    "    if RNN_architecture == \"LSTM\":\n",
    "        rnn_layer = Bidirectional(LSTM(units = units, return_sequences = False))(dropout1)\n",
    "    elif RNN_architecture == \"GRU\":\n",
    "        rnn_layer = Bidirectional(GRU(units = units, return_sequences = False))(dropout1)\n",
    "    elif RNN_architecture == \"RNN\":\n",
    "        rnn_layer = Bidirectional(SimpleRNN(units = units, return_sequences = False))(dropout1)\n",
    "    \n",
    "    dropout2 = Dropout(rate = dropout_rate)(rnn_layer)\n",
    "    batch = BatchNormalization()(dropout2)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network for Generating Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output layer network\n",
    "def output_channel(input_layer : layers ,activation : str, units : int, dropout_rate : float) -> layers:\n",
    "    \"\"\"\n",
    "    This function creates a sub network for outputing classification probabilities.\n",
    "    \n",
    "    Inputs:\n",
    "    input_layer - The input keras layer into the subnetwork\n",
    "    activation  - Name of the activation type to use\n",
    "    units - Number of units in the Dense network\n",
    "    dropout_rate - dropout rate\n",
    "    \n",
    "    Outputs:\n",
    "    output - Softmax output layer\n",
    "    \n",
    "    \"\"\"\n",
    "    assert activation in [\"ReLU\",\"PReLU\", \"ELU\", \"LeakyReLU\"]\n",
    "    \n",
    "    dense = Dense(units)(input_layer)\n",
    "    \n",
    "    if activation == \"PReLU\":\n",
    "        act = PReLU()(dense)\n",
    "    elif activation == \"ELU\":\n",
    "        act = ELU()(dense)\n",
    "    elif activation == \"LeakyReLU\":\n",
    "        act = LeakyReLU()(dense)\n",
    "    elif activation == \"ReLU\":\n",
    "        act = Dense(units, activation='relu')(input_layer)\n",
    "    \n",
    "        \n",
    "    dropout = Dropout(rate = dropout_rate)(act)\n",
    "    batch = BatchNormalization()(dropout)\n",
    "    output = Dense(3,activation='softmax', name = \"Output\")(batch)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put All Parts Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define full model.\n",
    "def define_model(RNN_architecture : str = \"LSTM\", rnn_units : int = 256, dense_units : int = 128,dense_activation : str = \"PReLU\" ,dropout_rate : float = 0.4) -> Model:\n",
    "    \"\"\"\n",
    "    This function defines and compiles a Multichannel RNN for Sentiment Classification.\n",
    "    \n",
    "    Inputs:\n",
    "    RNN_architecture - Name of the RNN type to use\n",
    "    rnn_units - Number of units in the RNN\n",
    "    dense_units - Number of units in the Dense network\n",
    "    dense_activation  - Name of the activation type to use\n",
    "    dropout_rate - dropout rate\n",
    "    \n",
    "    Outputs:\n",
    "    model - A Keras model\n",
    "    \n",
    "    \"\"\"\n",
    "    # Input Layer\n",
    "    shape = (MAX_SEQUENCE_LENGTH,)\n",
    "    input1 = Input(shape = shape, name = \"Main_input\")\n",
    "    \n",
    "    # Channel 1 - GLoVe\n",
    "    embedding1 = Embedding(len(word_index) + 1,\n",
    "              EMBEDDING_DIM,\n",
    "              weights=[glove_embedding_matrix],\n",
    "              input_length=MAX_SEQUENCE_LENGTH,\n",
    "              trainable=False,\n",
    "              input_shape=X_train.shape[1:], name = \"GLoVe_Embedding\")(input1)\n",
    "\n",
    "    net1 = subnetwork_channel(embedding1, RNN_architecture = RNN_architecture, units = rnn_units, dropout_rate = dropout_rate)\n",
    "    \n",
    "    # Channel 2 - Fast Text\n",
    "    embedding2 = Embedding(len(word_index) + 1,\n",
    "              EMBEDDING_DIM,\n",
    "              weights=[fasttext_embedding_matrix],\n",
    "              input_length=MAX_SEQUENCE_LENGTH,\n",
    "              trainable=False,\n",
    "              input_shape=shape, name = \"FastText_Embedding\")(input1)\n",
    "\n",
    "    net2 = subnetwork_channel(embedding2, RNN_architecture = RNN_architecture, units = rnn_units, dropout_rate = dropout_rate)\n",
    "    \n",
    "    # Merge\n",
    "    merged = concatenate([net1,net2], name =\"Merge\")\n",
    "    # Output channel\n",
    "    output = output_channel(merged, activation = dense_activation, units = dense_units, dropout_rate = dropout_rate)\n",
    "    \n",
    "    # Compile \n",
    "    model = Model(inputs = input1, outputs = output)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = Adam(0.002), metrics = ['categorical_accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/marketlab/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/marketlab/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Main_input (InputLayer)         (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "GLoVe_Embedding (Embedding)     (None, 32, 300)      116584800   Main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "FastText_Embedding (Embedding)  (None, 32, 300)      116584800   Main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 300)      0           GLoVe_Embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32, 300)      0           FastText_Embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 512)          1140736     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 512)          1140736     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 512)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 512)          2048        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 512)          2048        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Merge (Concatenate)             (None, 1024)         0           batch_normalization_1[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          131200      Merge[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128)          512         dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output (Dense)                  (None, 3)            387         batch_normalization_3[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 235,587,267\n",
      "Trainable params: 2,415,363\n",
      "Non-trainable params: 233,171,904\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = define_model(RNN_architecture = \"LSTM\",\n",
    "                     rnn_units= 256,\n",
    "                     dense_units = 128,\n",
    "                     dense_activation = \"ReLU\",\n",
    "                     dropout_rate = 0.4)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-a20b61d22e82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../imgs/multichannel-bidirectionalLSTM.png'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpic_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \"\"\"\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         raise ImportError(\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;34m'Failed to import `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;34m'Please install `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "pic_name = '../imgs/multichannel-bidirectionalLSTM.png'\n",
    "plot_model(model,show_shapes=True,to_file=pic_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/multichannel-bidirectionalLSTM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/marketlab/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "123061/123061 [==============================] - 25s 199us/step - loss: 0.7783 - categorical_accuracy: 0.6743\n",
      "Epoch 2/10\n",
      "123061/123061 [==============================] - 23s 183us/step - loss: 0.5505 - categorical_accuracy: 0.7604\n",
      "Epoch 3/10\n",
      "123061/123061 [==============================] - 23s 183us/step - loss: 0.5237 - categorical_accuracy: 0.7727\n",
      "Epoch 4/10\n",
      "123061/123061 [==============================] - 23s 184us/step - loss: 0.5095 - categorical_accuracy: 0.7791\n",
      "Epoch 5/10\n",
      "123061/123061 [==============================] - 23s 183us/step - loss: 0.4987 - categorical_accuracy: 0.7836\n",
      "Epoch 6/10\n",
      "123061/123061 [==============================] - 23s 183us/step - loss: 0.4910 - categorical_accuracy: 0.7890\n",
      "Epoch 7/10\n",
      "123061/123061 [==============================] - 23s 183us/step - loss: 0.4826 - categorical_accuracy: 0.7935\n",
      "Epoch 8/10\n",
      "123061/123061 [==============================] - 23s 185us/step - loss: 0.4767 - categorical_accuracy: 0.7958\n",
      "Epoch 9/10\n",
      "123061/123061 [==============================] - 23s 184us/step - loss: 0.4694 - categorical_accuracy: 0.7996\n",
      "Epoch 10/10\n",
      "123061/123061 [==============================] - 23s 183us/step - loss: 0.4592 - categorical_accuracy: 0.8039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6299935438>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train\n",
    "tensorboard = TensorBoard(log_dir='tasks/tensorboard/logs/')\n",
    "model.fit(X_train, y_train, epochs = 10, batch_size = 1024, callbacks = [tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Click [here](/tensorboard/) to start TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13674/13674 [==============================] - 17s 1ms/step\n",
      "Validation Accuracy: 79.0\n"
     ]
    }
   ],
   "source": [
    "## Find the testing accuracy\n",
    "val_loss, val_catergorical_accuracy = model.evaluate(X_test,y_test)\n",
    "print(\"Validation Accuracy: {:.1f}\".format(val_catergorical_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model was able to achieve ~79% accuracy. According to research on sentiment analysis and classification, human raters may only agree with each other about 80% of the time. Due to the nature of sentiment analysis, the outcome a reader arrives at can be very subjective depending on how the reader interprets the words, tone or phrasing of the text. Thus, a model that predicts with 100% accuracy may still disagree with a human 20% of the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Re-tune Neural Network Parameters\n",
    "Try experimenting with different parameters in the neural network.\n",
    "In the function 'define_model'\n",
    "    - 'RNN_architecture' can be one of: \"RNN\", \"GRU\", \"LSTM\".\n",
    "    - 'rnn_units' are the number of units in the RNN\n",
    "    - 'dense_units' are the number of units in the dense network\n",
    "    - 'dense_activation' can be one of: \"PReLU\", \"LeakyReLU\", \"ELU\", \"ReLU\"\n",
    "    - 'dropout_rate' rate of dropout throughout the network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
