{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing for Signal Generation on News Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of Natural Language Processing to News Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we will use the Keras + Tensorflow libraries to construct and train a multi-channel LSTM network for classifying sentiments.In the model below we will utilize pretrained embeddings from both GloVe and FastText Skipgram models.\n",
    "\n",
    "### Notebook Configurations and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import date\n",
    "from numpy.random import seed\n",
    "from IPython.display import Image\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import set_random_seed\n",
    "from tensorflow.python import pywrap_tensorflow\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(42)\n",
    "set_random_seed(42)\n",
    "MAX_SEQUENCE_LENGTH = 32\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "print(\"Keras version:\",keras.__version__)\n",
    "print(\"Tensorflow version:\",tf.__version__)\n",
    "print(\"Sklearn version:\",sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation - Financial News Dataset \n",
    "\n",
    "We will be utilizing open source news data from Bloomberg and Reuters between 2006 and 2012.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/news_data/news_data_labelled.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting timestamp: {}\".format(df.timestamp.min()))\n",
    "print(\"Ending timestamp: {}\".format(df.timestamp.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into testing and training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.tldr\n",
    "y = df.Class\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    stratify = y,\n",
    "                                                    test_size=0.10,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize training set and apply to train + test sets. Pad the sequences with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_filter = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "\n",
    "tokenizer = Tokenizer(num_words = None,\n",
    "                      filters = word_filter,\n",
    "                      lower = True,\n",
    "                      split = \" \",\n",
    "                      char_level = False)\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(tokenizer.texts_to_sequences(X_train),\n",
    "                        maxlen = MAX_SEQUENCE_LENGTH,\n",
    "                        value = 0.0)\n",
    "\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(X_test),\n",
    "                       maxlen = MAX_SEQUENCE_LENGTH,\n",
    "                       value = 0.0)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the embedding matrices for Glove and FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def embedding_matrix(path_to_embedding : str,embedding_dim: int, word_index : dict) -> np.array:\n",
    "    \"\"\"\n",
    "    This function creates an embedding matrix.\n",
    "    \n",
    "    Inputs:\n",
    "    path_to_embedding - path to text file of word embeddings\n",
    "    embedding_dim - dimension of word embeddings\n",
    "    word_index - dictionary mapping words to indices\n",
    "    \n",
    "    Outputs:\n",
    "    embedding_matrix - numpy matrix containing the embeddings\n",
    "    \n",
    "    \"\"\"\n",
    "    embeddings_index = {}\n",
    "    f = open(path_to_embedding, encoding='utf-8')\n",
    "    for line in f:\n",
    "        try:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1,embedding_dim))\n",
    "    found = 0\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            found +=1\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding_matrix = embedding_matrix(\"data/news_data/glove/glove.840B.300d.txt\",\n",
    "                                          EMBEDDING_DIM,\n",
    "                                          tokenizer.word_index)\n",
    "\n",
    "fasttext_embedding_matrix = embedding_matrix(\"data/news_data/fasttext/wiki-news-300d-1M.vec\",\n",
    "                                             EMBEDDING_DIM,\n",
    "                                             tokenizer.word_index)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
