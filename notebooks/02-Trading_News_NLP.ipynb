{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing for Signal Generation on News Data\n",
    "\n",
    "In the following code,  we will utilize pretrained embeddings from both GloVe and FastText Skipgram models to preprocess text datasets for LSTM network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages and Initialize the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import date\n",
    "from numpy.random import seed\n",
    "from IPython.display import Image\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import set_random_seed\n",
    "from tensorflow.python import pywrap_tensorflow\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/marketlab/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting random state to eliminate randomness. Assigning constant variables for later uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.2.4\n",
      "Tensorflow version: 1.13.1\n",
      "Sklearn version: 0.20.1\n"
     ]
    }
   ],
   "source": [
    "seed(42)\n",
    "set_random_seed(42)\n",
    "MAX_SEQUENCE_LENGTH = 32\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "print(\"Keras version:\",keras.__version__)\n",
    "print(\"Tensorflow version:\",tf.__version__)\n",
    "print(\"Sklearn version:\",sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data - Financial News Dataset \n",
    "\n",
    "We will be utilizing open source news data from Bloomberg and Reuters between 2006 and 2012.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "      <th>tldr</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exxon Mobil offers plan to end Alaska dispute</td>\n",
       "      <td>2006-10-20 06:15:00</td>\n",
       "      <td>http://www.reuters.com/article/2006/10/20/busi...</td>\n",
       "      <td>In a proposal sent earlier this week to the Al...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey buddy, can you spare $600 for a Google share?</td>\n",
       "      <td>2006-10-20 04:25:00</td>\n",
       "      <td>http://www.reuters.com/article/2006/10/20/busi...</td>\n",
       "      <td>SAN FRANCISCO/NEW YORK  (Reuters) - Wall Stree...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ford posts biggest loss in 14 years</td>\n",
       "      <td>2006-10-23 06:42:00</td>\n",
       "      <td>http://www.reuters.com/article/2006/10/23/us-a...</td>\n",
       "      <td>Ford also said it was considering raising new ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shell looks to buy out Canada unit for C$7.7 b...</td>\n",
       "      <td>2006-10-23 04:34:00</td>\n",
       "      <td>http://www.reuters.com/article/2006/10/23/us-e...</td>\n",
       "      <td>In July, Shell Canada rattled the industry and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. venture investors betting on energy, Web 2.0</td>\n",
       "      <td>2006-10-23 08:36:00</td>\n",
       "      <td>http://www.reuters.com/article/2006/10/23/us-f...</td>\n",
       "      <td>SAN FRANCISCO  (Reuters) - U.S. venture capita...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline            timestamp  \\\n",
       "0      Exxon Mobil offers plan to end Alaska dispute  2006-10-20 06:15:00   \n",
       "1  Hey buddy, can you spare $600 for a Google share?  2006-10-20 04:25:00   \n",
       "2                Ford posts biggest loss in 14 years  2006-10-23 06:42:00   \n",
       "3  Shell looks to buy out Canada unit for C$7.7 b...  2006-10-23 04:34:00   \n",
       "4  U.S. venture investors betting on energy, Web 2.0  2006-10-23 08:36:00   \n",
       "\n",
       "                                                 url  \\\n",
       "0  http://www.reuters.com/article/2006/10/20/busi...   \n",
       "1  http://www.reuters.com/article/2006/10/20/busi...   \n",
       "2  http://www.reuters.com/article/2006/10/23/us-a...   \n",
       "3  http://www.reuters.com/article/2006/10/23/us-e...   \n",
       "4  http://www.reuters.com/article/2006/10/23/us-f...   \n",
       "\n",
       "                                                tldr  Class  \n",
       "0  In a proposal sent earlier this week to the Al...      2  \n",
       "1  SAN FRANCISCO/NEW YORK  (Reuters) - Wall Stree...      1  \n",
       "2  Ford also said it was considering raising new ...      1  \n",
       "3  In July, Shell Canada rattled the industry and...      1  \n",
       "4  SAN FRANCISCO  (Reuters) - U.S. venture capita...      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/news_data/news_data_labelled.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         2\n",
      "1         1\n",
      "2         1\n",
      "3         1\n",
      "4         1\n",
      "5         2\n",
      "6         2\n",
      "7         2\n",
      "8         2\n",
      "9         2\n",
      "10        2\n",
      "11        1\n",
      "12        1\n",
      "13        1\n",
      "14        2\n",
      "15        1\n",
      "16        2\n",
      "17        2\n",
      "18        2\n",
      "19        2\n",
      "20        2\n",
      "21        2\n",
      "22        1\n",
      "23        1\n",
      "24        2\n",
      "25        1\n",
      "26        2\n",
      "27        1\n",
      "28        1\n",
      "29        2\n",
      "         ..\n",
      "136705    1\n",
      "136706    1\n",
      "136707    1\n",
      "136708    1\n",
      "136709    1\n",
      "136710    1\n",
      "136711    1\n",
      "136712    1\n",
      "136713    0\n",
      "136714    1\n",
      "136715    1\n",
      "136716    1\n",
      "136717    1\n",
      "136718    1\n",
      "136719    1\n",
      "136720    1\n",
      "136721    1\n",
      "136722    1\n",
      "136723    1\n",
      "136724    2\n",
      "136725    1\n",
      "136726    2\n",
      "136727    2\n",
      "136728    1\n",
      "136729    1\n",
      "136730    2\n",
      "136731    1\n",
      "136732    2\n",
      "136733    2\n",
      "136734    1\n",
      "Name: Class, Length: 136735, dtype: int64\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'value_counts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-deb5b24f7e43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#print(\"Starting timestamp: {}\".format(df.timestamp.min()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(\"Ending timestamp: {}\".format(df.timestamp.max()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Class\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'value_counts'"
     ]
    }
   ],
   "source": [
    "#print(\"Starting timestamp: {}\".format(df.timestamp.min()))\n",
    "#print(\"Ending timestamp: {}\".format(df.timestamp.max()))\n",
    "print(df[\"Class\"]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the dataset into testing and training datasets for machine learning**\n",
    "* X and y respectively correspond to data features (i.e. input) and data labels (i.e. output)\n",
    "  * Training set the data used to \"learn\" the parameters in our model with a supervised learning method. This usually uses the majority of the original dataset to achieve best effect.\n",
    "  * Testing set is the data used to evaluate the effectiveness of our model, often used to produce numerical metrics (e.g. accuracy rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.tldr\n",
    "y = df.Class\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    stratify = y,\n",
    "                                                    test_size=0.10,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "<img src=\"../imgs/preprocess_data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenize training set**\n",
    "\n",
    "* Tokenizer from Keras creates a vocabulary index from the training set based on word frequency.\n",
    "  * Tokenize here also did special character removal for us\n",
    "  * we can also perform stop word removal here\n",
    "* Every unique word is assigned a unique integer value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_filter = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "\n",
    "tokenizer = Tokenizer(num_words = None,\n",
    "                      filters = word_filter,\n",
    "                      lower = True,\n",
    "                      split = \" \",\n",
    "                      char_level = False)\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert all datasets to numerical values**\n",
    "* Apply the vocabulary index to X_train and X_test\n",
    "  * The datasets are converted from texts to sequences of integers based on previously created vocabulary index\n",
    "  * The sequences are padded with zeros and are limited with MAX_SEQUENCE_LENGTH to have a fixed length\n",
    "* Convert y_train and y_test to one-hot encoded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(tokenizer.texts_to_sequences(X_train),\n",
    "                        maxlen = MAX_SEQUENCE_LENGTH,\n",
    "                        value = 0.0)\n",
    "\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(X_test),\n",
    "                       maxlen = MAX_SEQUENCE_LENGTH,\n",
    "                       value = 0.0)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "print(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Embeding Matrix\n",
    "\n",
    "* In this project, we will use pretrained word embeding which are stored in files. We have to build the matrices from these files before we use them\n",
    "* Vocabulary index created earlier with the tokenizer is used to create the embedding matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def embedding_matrix(path_to_embedding : str,embedding_dim: int, word_index : dict) -> np.array:\n",
    "    \"\"\"\n",
    "    This function creates an embedding matrix.\n",
    "    \n",
    "    Inputs:\n",
    "    path_to_embedding - path to text file of word embeddings\n",
    "    embedding_dim - dimension of word embeddings\n",
    "    word_index - dictionary mapping words to indices\n",
    "    \n",
    "    Outputs:\n",
    "    embedding_matrix - numpy matrix containing the embeddings\n",
    "    \n",
    "    \"\"\"\n",
    "    embeddings_index = {}\n",
    "    f = open(path_to_embedding, encoding='utf-8')\n",
    "    for line in f:\n",
    "        try:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1,embedding_dim))\n",
    "    found = 0\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            found +=1\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/news_data/glove/glove.840B.300d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-944c71e3305e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m glove_embedding_matrix = embedding_matrix(\"data/news_data/glove/glove.840B.300d.txt\",\n\u001b[1;32m      2\u001b[0m                                           \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                           tokenizer.word_index)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m fasttext_embedding_matrix = embedding_matrix(\"data/news_data/fasttext/wiki-news-300d-1M.vec\",\n",
      "\u001b[0;32m<ipython-input-11-96471f7f00e8>\u001b[0m in \u001b[0;36membedding_matrix\u001b[0;34m(path_to_embedding, embedding_dim, word_index)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \"\"\"\n\u001b[1;32m     14\u001b[0m     \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/news_data/glove/glove.840B.300d.txt'"
     ]
    }
   ],
   "source": [
    "glove_embedding_matrix = embedding_matrix(\"data/news_data/glove/glove.840B.300d.txt\",\n",
    "                                          EMBEDDING_DIM,\n",
    "                                          tokenizer.word_index)\n",
    "\n",
    "fasttext_embedding_matrix = embedding_matrix(\"data/news_data/fasttext/wiki-news-300d-1M.vec\",\n",
    "                                             EMBEDDING_DIM,\n",
    "                                             tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
