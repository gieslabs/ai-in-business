{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing for Signal Generation on News Data\n",
    "\n",
    "In the following code,  we will utilize pretrained embeddings from both GloVe and FastText Skipgram models to preprocess text datasets for LSTM network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages and Initialize the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import datetime\n",
    "import pydot, graphviz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import date\n",
    "from numpy.random import seed\n",
    "from IPython.display import Image\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import set_random_seed\n",
    "from tensorflow.python import pywrap_tensorflow\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM, concatenate, Bidirectional\n",
    "from keras.layers import PReLU, ELU, LeakyReLU, GRU, SimpleRNN\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Embedding, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/marketlab/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tradingcore.utils import embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting random state to eliminate randomness. Assigning constant variables for later uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.2.4\n",
      "Tensorflow version: 1.13.1\n",
      "Sklearn version: 0.20.1\n"
     ]
    }
   ],
   "source": [
    "#seed(42)\n",
    "#set_random_seed(42)\n",
    "MAX_SEQUENCE_LENGTH = 256\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "print(\"Keras version:\",keras.__version__)\n",
    "print(\"Tensorflow version:\",tf.__version__)\n",
    "print(\"Sklearn version:\",sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data - Financial News Dataset \n",
    "Contributors of this dataset viewed a new article headline and a short, bolded excerpt of a sentence or two from the attendant article. Next, they decided if the sentence in question provided an indication of the U.S. economy’s health, then rated the indication on a scale of 1-9, with 1 being negative and 9 being positive.\n",
    "source: https://www.figure-eight.com/data-for-everyone/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>positivity</th>\n",
       "      <th>positivity:confidence</th>\n",
       "      <th>relevance</th>\n",
       "      <th>relevance:confidence</th>\n",
       "      <th>orig__golden</th>\n",
       "      <th>articleid</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>lineid</th>\n",
       "      <th>next_sentence</th>\n",
       "      <th>positivity_gold</th>\n",
       "      <th>previous_sentence</th>\n",
       "      <th>relevance_gold</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>729487630</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>6/8/15 14:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>not sure</td>\n",
       "      <td>0.3469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109092213</td>\n",
       "      <td>2/23/93</td>\n",
       "      <td>Nasdaq Index Falls 1.7% But Dow Stocks Are Up:...</td>\n",
       "      <td>109092213_01</td>\n",
       "      <td>The Nasdaq composite index, home of technology...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The stock market accelerated its screeching sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>729487631</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>6/11/15 10:58</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.3675</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109092213</td>\n",
       "      <td>2/23/93</td>\n",
       "      <td>Nasdaq Index Falls 1.7% But Dow Stocks Are Up:...</td>\n",
       "      <td>109092213_02</td>\n",
       "      <td>The bond market continued to rally, propelling...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The stock market accelerated its screeching sw...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Nasdaq composite index, home of technology...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>729487632</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>6/6/15 0:15</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.3416</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.6771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109092213</td>\n",
       "      <td>2/23/93</td>\n",
       "      <td>Nasdaq Index Falls 1.7% But Dow Stocks Are Up:...</td>\n",
       "      <td>109092213_03</td>\n",
       "      <td>The Nasdaq market was stricken by the collapse...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Nasdaq composite index, home of technology...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The bond market continued to rally, propelling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>729487633</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>6/14/15 20:27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.6756</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109092213</td>\n",
       "      <td>2/23/93</td>\n",
       "      <td>Nasdaq Index Falls 1.7% But Dow Stocks Are Up:...</td>\n",
       "      <td>109092213_04</td>\n",
       "      <td>uring the marketÛªs split ÛÓ was 4.26 percen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The bond market continued to rally, propelling...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Nasdaq market was stricken by the collapse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>729487634</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>6/6/15 13:22</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6509</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109092213</td>\n",
       "      <td>2/23/93</td>\n",
       "      <td>Nasdaq Index Falls 1.7% But Dow Stocks Are Up:...</td>\n",
       "      <td>109092213_05</td>\n",
       "      <td>. \"It's a nervous market,Û said Lawrence R. ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Nasdaq market was stricken by the collapse...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uring the marketÛªs split ÛÓ was 4.26 percen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  729487630    False   finalized                   3      6/8/15 14:26   \n",
       "1  729487631    False   finalized                   3     6/11/15 10:58   \n",
       "2  729487632    False   finalized                   3       6/6/15 0:15   \n",
       "3  729487633    False   finalized                   3     6/14/15 20:27   \n",
       "4  729487634    False   finalized                   3      6/6/15 13:22   \n",
       "\n",
       "   positivity  positivity:confidence relevance  relevance:confidence  \\\n",
       "0         NaN                 0.0000  not sure                0.3469   \n",
       "1         6.0                 0.3675       yes                1.0000   \n",
       "2         5.0                 0.3416       yes                0.6771   \n",
       "3         4.0                 0.6756       yes                1.0000   \n",
       "4         3.0                 0.6509       yes                1.0000   \n",
       "\n",
       "  orig__golden  articleid     date  \\\n",
       "0          NaN  109092213  2/23/93   \n",
       "1          NaN  109092213  2/23/93   \n",
       "2          NaN  109092213  2/23/93   \n",
       "3          NaN  109092213  2/23/93   \n",
       "4          NaN  109092213  2/23/93   \n",
       "\n",
       "                                            headline        lineid  \\\n",
       "0  Nasdaq Index Falls 1.7% But Dow Stocks Are Up:...  109092213_01   \n",
       "1  Nasdaq Index Falls 1.7% But Dow Stocks Are Up:...  109092213_02   \n",
       "2  Nasdaq Index Falls 1.7% But Dow Stocks Are Up:...  109092213_03   \n",
       "3  Nasdaq Index Falls 1.7% But Dow Stocks Are Up:...  109092213_04   \n",
       "4  Nasdaq Index Falls 1.7% But Dow Stocks Are Up:...  109092213_05   \n",
       "\n",
       "                                       next_sentence positivity_gold  \\\n",
       "0  The Nasdaq composite index, home of technology...             NaN   \n",
       "1  The bond market continued to rally, propelling...             NaN   \n",
       "2  The Nasdaq market was stricken by the collapse...             NaN   \n",
       "3  uring the marketÛªs split ÛÓ was 4.26 percen...             NaN   \n",
       "4  . \"It's a nervous market,Û said Lawrence R. ...             NaN   \n",
       "\n",
       "                                   previous_sentence relevance_gold  \\\n",
       "0                                                NaN            NaN   \n",
       "1  The stock market accelerated its screeching sw...            NaN   \n",
       "2  The Nasdaq composite index, home of technology...            NaN   \n",
       "3  The bond market continued to rally, propelling...            NaN   \n",
       "4  The Nasdaq market was stricken by the collapse...            NaN   \n",
       "\n",
       "                                                text  \n",
       "0  The stock market accelerated its screeching sw...  \n",
       "1  The Nasdaq composite index, home of technology...  \n",
       "2  The bond market continued to rally, propelling...  \n",
       "3  The Nasdaq market was stricken by the collapse...  \n",
       "4  uring the marketÛªs split ÛÓ was 4.26 percen...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/us-economic-newspaper.csv\",encoding = \"ISO-8859-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the dataset into testing and training datasets for machine learning**\n",
    "* X and y respectively correspond to data features (i.e. input) and data labels (i.e. output)\n",
    "  * Training set the data used to \"learn\" the parameters in our model with a supervised learning method. This usually uses the majority of the original dataset to achieve best effect.\n",
    "  * Testing set is the data used to evaluate the effectiveness of our model, often used to produce numerical metrics (e.g. accuracy rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.text\n",
    "y = df.positivity\n",
    "y = y.fillna(5)\n",
    "for i, score in enumerate(y):\n",
    "    if score > 5: \n",
    "        y[i] = 2\n",
    "    elif score == 5:\n",
    "        y[i] = 1\n",
    "    else:\n",
    "        y[i] = 0\n",
    "y = y.astype('int32')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    stratify = y,\n",
    "                                                    test_size=0.10,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "<img src=\"../imgs/preprocess_data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenize training set**\n",
    "\n",
    "* Tokenizer from Keras creates a vocabulary index from the training set based on word frequency.\n",
    "  * Tokenize here also did special character removal for us\n",
    "  * we can also perform stop word removal here\n",
    "* Every unique word is assigned a unique integer value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_filter = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "\n",
    "tokenizer = Tokenizer(num_words = None,\n",
    "                      filters = word_filter,\n",
    "                      lower = True,\n",
    "                      split = \" \",\n",
    "                      char_level = False)\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert all datasets to numerical values**\n",
    "* Apply the vocabulary index to X_train and X_test\n",
    "  * The datasets are converted from texts to sequences of integers based on previously created vocabulary index\n",
    "  * The sequences are padded with zeros and are limited with MAX_SEQUENCE_LENGTH to have a fixed length\n",
    "* Convert y_train and y_test to one-hot encoded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(tokenizer.texts_to_sequences(X_train),\n",
    "                        maxlen = MAX_SEQUENCE_LENGTH,\n",
    "                        value = 0.0)\n",
    "\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(X_test),\n",
    "                       maxlen = MAX_SEQUENCE_LENGTH,\n",
    "                       value = 0.0)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Embeding Matrix\n",
    "\n",
    "In this project, we will use pretrained word embeding which are stored in files. We have to build the matrices from these files before we use them. Vocabulary index created earlier with the tokenizer is used to create the embedding matrices **embedding_matrix** is a function defined in utils.py which helps us build word embeding from files. It has the following parameters:\n",
    "* path_to_embedding： path to text file of word embeddings\n",
    "* embedding_dim: dimension of word embeddings\n",
    "* word_index: dictionary mapping words to indices\n",
    "\n",
    "The output is a numpy matrix containing the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding_matrix = embedding_matrix(\"../data/news_data/glove/glove.840B.300d.txt\",\n",
    "                                          EMBEDDING_DIM,\n",
    "                                          tokenizer.word_index)\n",
    "\n",
    "fasttext_embedding_matrix = embedding_matrix(\"../data/news_data/fasttext/wiki-news-300d-1M.vec\",\n",
    "                                             EMBEDDING_DIM,\n",
    "                                             tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks Model\n",
    "Recall from the first notebook that Nerual Network can be treated as a Composite function and each layer is one of these ingredient functions. When we write code to build our own Nerual Network using Keras, what we have to do is just find the layer we want to use and compose (or say concatenate) them. For example, the following network:\n",
    "<img src=\"../imgs/layer_example.png\">\n",
    "will be encoded as:\n",
    "<img src=\"../imgs/layer_code.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Network for processing sequential information**\n",
    "<img src=\"../imgs/sequence_subnet.png\">\n",
    "The function above build the subnet for processing sequential information. The sequencial information in this project is the sentences represented by a sequence of embeded word vectors. Keras has already provided a LSTM/GRU/SimpleRNN class that we could use a black box to represent the entire LSTM/GRU/RNN network.\n",
    "\n",
    "Things you may want to know:\n",
    "* difference between LSTM/GRU/RNN (how to choose which one to use): \n",
    "    * model performance: LSTM > GRU > RNN\n",
    "    * memory requirement: LSTM > GRU > RNN\n",
    "    * Speed of training and predicting: RNN > GRU > LSTM\n",
    "    * Rule of thumb: Pick LSTM if you want better model performance. Pick RNN if you care more about speed or don't have enough computing resources.\n",
    "* overfitting: the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably.  \n",
    "* Dropout layer: Randomly zero out some values in the input. Reduce overfitting.\n",
    "* Batch Normalization layer: output the normalized input. Prevent extreme value and reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subchannel network for encoding sequential information\n",
    "def subnetwork_channel(input_layer : layers,\n",
    "                       RNN_architecture : str,\n",
    "                       units : int,\n",
    "                       dropout_rate : float) -> layers:\n",
    "    \"\"\"\n",
    "    This function creates a sub network for encoding sequences.\n",
    "    \n",
    "    Inputs:\n",
    "    input_layer - The input keras layer into the subnetwork\n",
    "    RNN_architecture - Name of the RNN type to use\n",
    "    units - Number of units in the RNN\n",
    "    dropout_rate - dropout rate\n",
    "    \n",
    "    Outputs:\n",
    "    batch - Batch Normalized output layer\n",
    "    \n",
    "    \"\"\"\n",
    "    assert RNN_architecture in [\"LSTM\", \"GRU\", \"RNN\"]\n",
    "    \n",
    "    dropout1 = Dropout(rate = dropout_rate)(input_layer)\n",
    "    \n",
    "    if RNN_architecture == \"LSTM\":\n",
    "        rnn_layer = Bidirectional(LSTM(units = units, return_sequences = False))(dropout1)\n",
    "    elif RNN_architecture == \"GRU\":\n",
    "        rnn_layer = Bidirectional(GRU(units = units, return_sequences = False))(dropout1)\n",
    "    elif RNN_architecture == \"RNN\":\n",
    "        rnn_layer = Bidirectional(SimpleRNN(units = units, return_sequences = False))(dropout1)\n",
    "    \n",
    "    dropout2 = Dropout(rate = dropout_rate)(rnn_layer)\n",
    "    batch = BatchNormalization()(dropout2)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network for Generating Signal\n",
    "This sub-network takes in the feature vectors of a piece of text and predict (? what is the meaning of the class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output layer network\n",
    "def output_channel(input_layer : layers,\n",
    "                   activation : str,\n",
    "                   units : int,\n",
    "                   dropout_rate : float) -> layers:\n",
    "    \"\"\"\n",
    "    This function creates a sub network for outputing classification probabilities.\n",
    "    \n",
    "    Inputs:\n",
    "    input_layer - The input keras layer into the subnetwork\n",
    "    activation  - Name of the activation type to use\n",
    "    units - Number of units in the Dense network\n",
    "    dropout_rate - dropout rate\n",
    "    \n",
    "    Outputs:\n",
    "    output - Softmax output layer\n",
    "    \n",
    "    \"\"\"\n",
    "    assert activation in [\"ReLU\",\"PReLU\", \"ELU\", \"LeakyReLU\"]\n",
    "    \n",
    "    dense = Dense(units)(input_layer)\n",
    "    \n",
    "    if activation == \"PReLU\":\n",
    "        act = PReLU()(dense)\n",
    "    elif activation == \"ELU\":\n",
    "        act = ELU()(dense)\n",
    "    elif activation == \"LeakyReLU\":\n",
    "        act = LeakyReLU()(dense)\n",
    "    elif activation == \"ReLU\":\n",
    "        act = Dense(units, activation='relu')(input_layer)\n",
    "    \n",
    "        \n",
    "    dropout = Dropout(rate = dropout_rate)(act)\n",
    "    batch = BatchNormalization()(dropout)\n",
    "    output = Dense(3,activation='softmax', name = \"Output\")(batch)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put All Parts Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define full model.\n",
    "def define_model(RNN_architecture : str = \"LSTM\",\n",
    "                 rnn_units : int = 256,\n",
    "                 dense_units : int = 128,\n",
    "                 dense_activation : str = \"PReLU\",\n",
    "                 dropout_rate : float = 0.4) -> Model:\n",
    "    \"\"\"\n",
    "    This function defines and compiles a Multichannel RNN for Sentiment Classification.\n",
    "    \n",
    "    Inputs:\n",
    "    RNN_architecture - Name of the RNN type to use\n",
    "    rnn_units - Number of units in the RNN\n",
    "    dense_units - Number of units in the Dense network\n",
    "    dense_activation  - Name of the activation type to use\n",
    "    dropout_rate - dropout rate\n",
    "    \n",
    "    Outputs:\n",
    "    model - A Keras model\n",
    "    \n",
    "    \"\"\"\n",
    "    # Input Layer\n",
    "    shape = (MAX_SEQUENCE_LENGTH,)\n",
    "    input1 = Input(shape = shape, name = \"Main_input\")\n",
    "    \n",
    "    # Channel 1 - GLoVe\n",
    "    embedding1 = Embedding(len(word_index) + 1,\n",
    "              EMBEDDING_DIM,\n",
    "              weights=[glove_embedding_matrix],\n",
    "              input_length=MAX_SEQUENCE_LENGTH,\n",
    "              trainable=False,\n",
    "              input_shape=X_train.shape[1:], name = \"GLoVe_Embedding\")(input1)\n",
    "\n",
    "    net1 = subnetwork_channel(embedding1,\n",
    "                              RNN_architecture = RNN_architecture,\n",
    "                              units = rnn_units,\n",
    "                              dropout_rate = dropout_rate)\n",
    "    \n",
    "    # Channel 2 - Fast Text\n",
    "    embedding2 = Embedding(len(word_index) + 1,\n",
    "              EMBEDDING_DIM,\n",
    "              weights=[fasttext_embedding_matrix],\n",
    "              input_length=MAX_SEQUENCE_LENGTH,\n",
    "              trainable=False,\n",
    "              input_shape=shape, name = \"FastText_Embedding\")(input1)\n",
    "\n",
    "    net2 = subnetwork_channel(embedding2,\n",
    "                              RNN_architecture = RNN_architecture,\n",
    "                              units = rnn_units, \n",
    "                              dropout_rate = dropout_rate)\n",
    "    \n",
    "    # Merge\n",
    "    merged = concatenate([net1,net2], name =\"Merge\")\n",
    "    # Output channel\n",
    "    output = output_channel(merged,\n",
    "                            activation = dense_activation,\n",
    "                            units = dense_units,\n",
    "                            dropout_rate = dropout_rate)\n",
    "    \n",
    "    # Compile \n",
    "    model = Model(inputs = input1, outputs = output)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = Adam(0.002), metrics = ['categorical_accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Model\n",
    "you can change the parameters here and try to get better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Main_input (InputLayer)         (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "GLoVe_Embedding (Embedding)     (None, 256, 300)     5474700     Main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "FastText_Embedding (Embedding)  (None, 256, 300)     5474700     Main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 256, 300)     0           GLoVe_Embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 256, 300)     0           FastText_Embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_21 (Bidirectional (None, 512)          1140736     dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_22 (Bidirectional (None, 512)          1140736     dropout_53[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 512)          0           bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, 512)          0           bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 512)          2048        dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 512)          2048        dropout_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Merge (Concatenate)             (None, 1024)         0           batch_normalization_31[0][0]     \n",
      "                                                                 batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 128)          131200      Merge[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 128)          0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 128)          512         dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Output (Dense)                  (None, 3)            387         batch_normalization_33[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 13,367,067\n",
      "Trainable params: 2,415,363\n",
      "Non-trainable params: 10,951,704\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = define_model(RNN_architecture = \"LSTM\",\n",
    "                     rnn_units= 256,\n",
    "                     dense_units = 128,\n",
    "                     dense_activation = \"ReLU\",\n",
    "                     dropout_rate = 0.4)\n",
    "\n",
    "# print out a summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/multichannel-bidirectionalLSTM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Fit the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4513/4513 [==============================] - 18s 4ms/step - loss: 1.5048 - categorical_accuracy: 0.3705\n",
      "Epoch 2/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 1.3141 - categorical_accuracy: 0.4124\n",
      "Epoch 3/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 1.1887 - categorical_accuracy: 0.4620\n",
      "Epoch 4/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 1.1140 - categorical_accuracy: 0.4853\n",
      "Epoch 5/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 1.0614 - categorical_accuracy: 0.5121\n",
      "Epoch 6/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 1.0338 - categorical_accuracy: 0.5192\n",
      "Epoch 7/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.9872 - categorical_accuracy: 0.5360\n",
      "Epoch 8/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.9745 - categorical_accuracy: 0.5400\n",
      "Epoch 9/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.9452 - categorical_accuracy: 0.5595\n",
      "Epoch 10/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.9200 - categorical_accuracy: 0.5604\n",
      "Epoch 11/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.9102 - categorical_accuracy: 0.5801\n",
      "Epoch 12/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.8798 - categorical_accuracy: 0.5930\n",
      "Epoch 13/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.8899 - categorical_accuracy: 0.5828\n",
      "Epoch 14/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.8879 - categorical_accuracy: 0.5832\n",
      "Epoch 15/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.8882 - categorical_accuracy: 0.5814\n",
      "Epoch 16/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.8838 - categorical_accuracy: 0.5996\n",
      "Epoch 17/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.8625 - categorical_accuracy: 0.5952\n",
      "Epoch 18/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.8471 - categorical_accuracy: 0.6018\n",
      "Epoch 19/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.8539 - categorical_accuracy: 0.5994\n",
      "Epoch 20/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.8201 - categorical_accuracy: 0.6147\n",
      "Epoch 21/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.8211 - categorical_accuracy: 0.6178\n",
      "Epoch 22/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.8024 - categorical_accuracy: 0.6331\n",
      "Epoch 23/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.7984 - categorical_accuracy: 0.6364\n",
      "Epoch 24/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.7811 - categorical_accuracy: 0.6459\n",
      "Epoch 25/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.7596 - categorical_accuracy: 0.6563\n",
      "Epoch 26/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.7519 - categorical_accuracy: 0.6608\n",
      "Epoch 27/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.7426 - categorical_accuracy: 0.6683\n",
      "Epoch 28/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.7269 - categorical_accuracy: 0.6752\n",
      "Epoch 29/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.7051 - categorical_accuracy: 0.6865\n",
      "Epoch 30/30\n",
      "4513/4513 [==============================] - 16s 3ms/step - loss: 0.6943 - categorical_accuracy: 0.6865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd7f39e6c50>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train\n",
    "tensorboard = TensorBoard(log_dir='tasks/tensorboard/logs/2/')\n",
    "model.fit(X_train, y_train, epochs = 30, batch_size = 1024, callbacks = [tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Click [here](/tensorboard/) to start TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing\n",
    "Test the model on data that are not in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502/502 [==============================] - 7s 14ms/step\n",
      "Validation Accuracy: 57.4\n"
     ]
    }
   ],
   "source": [
    "## Find the testing accuracy\n",
    "val_loss, val_catergorical_accuracy = model.evaluate(X_test,y_test)\n",
    "print(\"Validation Accuracy: {:.1f}\".format(val_catergorical_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model was able to achieve ~79% accuracy. According to research on sentiment analysis and classification, human raters may only agree with each other about 80% of the time. Due to the nature of sentiment analysis, the outcome a reader arrives at can be very subjective depending on how the reader interprets the words, tone or phrasing of the text. Thus, a model that predicts with 100% accuracy may still disagree with a human 20% of the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Re-tune Neural Network Parameters\n",
    "Try experimenting with different parameters in the neural network.\n",
    "In the function 'define_model'\n",
    "    - 'RNN_architecture' can be one of: \"RNN\", \"GRU\", \"LSTM\".\n",
    "    - 'rnn_units' are the number of units in the RNN\n",
    "    - 'dense_units' are the number of units in the dense network\n",
    "    - 'dense_activation' can be one of: \"PReLU\", \"LeakyReLU\", \"ELU\", \"ReLU\"\n",
    "    - 'dropout_rate' rate of dropout throughout the network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
